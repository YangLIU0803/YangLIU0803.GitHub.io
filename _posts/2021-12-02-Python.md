---
layout: article
title: 在学校做过的Python
key: 10003
tags: Python
category: blog
date: 2021-12-02 14:50:00 +08:00
modify_date: 2021-12-03 11:40:00 +08:00
picture_frame: shadow
---
想要整理一下曾经在学校中做过的python数据处理及可视化项目。
以及想念zjc老师。

**还在更新中。**
<!--more-->

### 1 词云
slides-2
– 针对一个文本文档，读入并分词，过滤停用词，
统计所有词的频率，排序输出前topn个词。
– 对所有出现的词依一定规则进行过滤，得到特
征词，将文档用特征词表示为向量。
– 可视化为关键词云。
– 待分析的文本文件：doc1.txt
– 停用词表： stopwords_list.txt
– 代码：w2demo1.py
根据提供的评论数据(online_reviews_texts.txt，见资源/data，一行一条评论，因此一行可以视为一个文档，行号可以作为文档编号），读入所有文档并分词，统计词频，找到高频词，确实特征集，为每一条评论生成向量表示，计算一下不同评论之间的距离（自定义，如欧氏或余弦），能不能找到所有评论的“重心”或者所有评论中的代表性评论并输出原文？除了词云外，针对多文档数据还有别的可视化方式没有？

### 2 情绪理解
slides-3
情绪理解是文本挖掘里最常见任务之一。现提供一个简单的五类情绪字
典（由情绪词组成，5个文件），并利用该字典对近5000条新浪微博进
行情绪理解（一行一条微博），比如那类情绪词最多，则为那类情绪。
同时实现分析结果的可视化，因为每条微博提供了经纬度和发布时间，
可以用颜色区分不同情绪，借以绘制情绪的时空分布。
– 注意火星坐标及其转换（网上应该有资料）
– 注意用函数封装代码
– 注意分词时要将情绪字典加入到jieba自定义词典
– 注意观察微博数据的特征以及对情绪理解方法的影响
• 短、噪声、口语表达等
– 注意异常情况的处理
• 无情绪词出现，不同情绪的情绪词出现数目一样等
– 注意考虑和总结字典方法的缺点
• 有无可能进一步扩充字典，人工之外，有无别的方法
– 相关文件
• 字典：资源/data/emotion_dictionary_test.zip
• 微博：资源/data/weibo_test.txt

### 3 文件处理结构化数据
slides-4
经济管理中通常有大量的数据以excel格式存在(信管除外)，如本次作业提供
的中国各省长周期CO2排放数据。
– 资源/data/co2_demo.zip
– 对该数据进行自定义的统计分析与理解。比如观察随年份各省CO2排放变化趋势，
不同省份变化趋势的相似性（能否简单解释），拐点年份（由上升变成下降或相
反）及其空间分布规律（如区域性的东西不同等），以及对变化趋势的量化解释
（产业分布、产业转移等视角，需要有简单数据分析支持）。
• 可以仅关注重点省份（太好或太差）
• 按自己的理解和时间安排展开，工作量可大可小，不作太多要求。
– 思考并简述一下相关结果的所谓“政策洞见”。

– 请分模块实现，如将数据读取并返回的相关功能封装为一个模块，对数据进行统
计分析的相关功能封装为一个模块，对数据进行曲线、地图等可视化功能封装为
一个模块，最后编写一个主模块，集成其他模块。
– 在主模块中，尝试将分析结果（如分布、趋势、相似性等）序列化，在可视化时
可考虑反序列化数据并使用。
– 体会此类结构化数据处理与非结构化数据的差异。机会难得，因为我们后面不会
再处理这类简单的结构化数据了。
– 推荐包：xlrd, pandas, seaborn, pyecharts等

### 4 词网networkx Gephi
slides-6
通过构建词网来比较好评跟差评在用词、话题、用语习惯上的可能差别。
– 提供京东计算机类产品的5分评论和1分评论
• 资源/data/1分和5分评论文本.txt
• 一行一条评论，内容和分值用\t隔开,共20000条，各10000条
– 实现一个类WordNetwork，继承networkx里的Graph，并实现新的生成词网络
的方法，当两个词在一条评论中的距离小于w时，认为两个词之间有语义关联并建
立边（滑动窗口）
– 实现边的过滤方法，即当两个词共现的次数小于t时，应该滤去，认为语义相关性
不显著
– 比较5分评论与1分评论词网的差别
• 用结构属性对词排序(degree, hits, pagerank)，看共有词位置的差别
– 比如看看topn的词的差别
– 比如对所有共有词，产生一个反映重要性的归一化的向量，比较两个向量的差异（相关性小等）
• 可视化共有词（或关键共有词）的连接结构有何差异（用networkx库里的可视化方法，
或者直接用Gephi工具）
– 利用networkx里的社团发现方法看看不同评论里的话题分布
• 话题一般会体现为由一些连接紧密的词组成的子结构
• 尝试用Gephi可视化
– 当调整w（一般认为w=10以内）和t的时候，上述差异等比较有何变化？
– 其他一些思考
• 了解一下DeepWalk，如何在网络上对词的语义进行“嵌入”表达并将文本转成向量空间
• 如何度量两个词的相似性，通过结构
• 如果有时间信息，怎么通过结构的变化来量化语义的“时光变迁”(drift)

### 5 图像识别
slides-8
装饰器的使用：
根据提供的数据实现一个图片检索类，输入一张图片，展示数据库中最相似的前n张照片。
– 数据:资源/icon.zip,约4000张人脸
– 建议使用的库：Pillow, imagehash, heapq
– 要点：如何计算图片的相似性；如何实现快速的比对。
– 注意：图片数据应该只导入内存一次，避免每次检索都要重新导入图片；实现统计一次检索耗费时间的装饰器；检索应有阈值要求（如相似性大于某值时定义为不相似），当在对应阈值下无检索结果时抛出自定义异常并在上层调用时处理；
– 附加：可以考虑先检测人脸，仅对脸部进行检索匹配，可能会压缩数据并提高准确率。
– Demo: itest.py, itest2.py, itest3.py
